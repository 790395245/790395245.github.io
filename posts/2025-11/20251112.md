---
title: Python小技巧：对象序列化、BaseSettings配置管理、Celery任务队列
date: 2025-11-12
categories:
  - 技术分享
tags:
  - python
  - 实用技巧
description: Python小技巧
articleGPT: 介绍了Python的几个小技巧包括对象序列化、BaseSettings配置管理、Celery任务队列
top:
share: true
delete: false
cover: https://cdn.rz15.cn/uploads/2025/07/ca32b71bc28c8f446c8c382083516905.jpeg
---

# 工作经验分享

## Python中的对象序列化和反序列化
在保存数据到文件时，由于不同格式的设计目标和规则不同，Python（或对应的处理库）常会对原始数据做一些“隐性修改”，导致重新读取时和原数据不完全一致。以下是一些常见例子：


### 1. 保存为文本格式（如JSON）时的类型转换
JSON是文本格式，只支持有限的数据类型（字符串、数字、布尔、null、列表、字典），Python中其他类型会被自动转换：
- **整数/浮点数混淆**：  
  原始数据是 `int` 类型（如 `10`），保存为JSON后再读取，可能变成 `float` 类型（`10.0`）（部分库会自动处理，但极端情况可能出错）。
- **元组变列表**：  
  原始数据 `(1, 2, 3)`（元组），JSON没有“元组”类型，保存后会变成 `[1, 2, 3]`（列表），读取后无法恢复为元组。
- **字符串编码**：  
  包含特殊字符的字符串（如 `'a\tb'` 中的制表符），保存为JSON时会被转义为 `'a\\tb'`，读取时需手动处理才能恢复原始格式。


### 2. 保存为CSV时的格式丢失
CSV适合简单表格数据，但会丢失很多信息：
- **数据类型统一为字符串**：  
  原始数据中混合的 `int`（如 `123`）、`float`（如 `3.14`）、`bool`（如 `True`），保存到CSV后都会变成字符串（`'123'`、`'3.14'`、`'True'`），读取时需要手动转换类型。
- **列表/字典被“扁平化”**：  
  原始数据中的复杂结构（如 `[1, 2, 3]` 或 `{'a': 1}`），保存到CSV时会被直接转为字符串（如 `'[1, 2, 3]'`），读取后无法直接恢复为列表/字典，需要额外解析。

### 为什么pickle在复现场景中更可靠？
正因为上述格式会“修改”原始数据，所以在需要**精确复现数据（包括类型、结构、甚至自定义对象）** 的场景（如测试、调试、存档），`pickle` 更常用——它会完整记录对象的“原貌”，包括类型、值、甚至类的定义信息，读取后几乎能100%还原原始数据（前提是环境一致）。

但反之，如果你需要数据被其他语言或工具读取，`pickle` 就不适合了（因为它是Python专属的）。


```python
import pickle

# 1. 准备一个要序列化的对象
data_to_save = {
    'name': 'Alice',
    'scores': [100, 95, 88],
    'is_active': True,
    'settings': {'theme': 'dark', 'level': 5}
}

# 2. 序列化：将对象 'dump' (倾倒) 到文件中
# 注意：必须使用 'wb' (write binary) 二进制写入模式
with open('data.pkl', 'wb') as file:
    pickle.dump(data_to_save, file)

print("数据已保存到 data.pkl")

# ... 假设程序关闭后重新打开 ...

# 3. 反序列化：从文件中 'load' (加载) 对象
# 注意：必须使用 'rb' (read binary) 二进制读取模式
with open('data.pkl', 'rb') as file:
    loaded_data = pickle.load(file)

print("从 data.pkl 加载的数据:")
print(loaded_data)

# 4. 验证数据
print(f"类型是否相同: {type(loaded_data) == type(data_to_save)}")
print(f"内容是否相同: {loaded_data == data_to_save}")
```

## BaseSettings 简化配置管理

在 Python 中，`BaseSettings` 是 **Pydantic** 库（v1 及以上版本）提供的一个核心类，主要用于 **配置管理**。它允许开发者通过类定义的方式声明配置项，自动处理配置的加载、验证、类型转换等功能，尤其适合从环境变量、配置文件（如 `.env`）中读取项目配置，大幅简化配置管理流程。


### **核心作用**
1. **配置定义与验证**：通过类属性声明配置项的类型和默认值，自动校验配置值的类型和合法性。
2. **多源配置加载**：支持从环境变量、`.env` 文件、字典等多种来源加载配置，且优先级可控制（环境变量通常覆盖默认值）。
3. **类型自动转换**：将环境变量中读取的字符串自动转换为声明的类型（如 `int`、`bool`、`list` 等）。
4. **代码提示友好**：基于类定义，支持 IDE 自动补全和类型检查，提升开发效率。


### **使用方法**
#### 1. 安装 Pydantic
首先需要安装 Pydantic（以 v2 为例，v1 用法类似但部分细节有差异）：
```bash
pip install pydantic
```


#### 2. 基本用法：定义配置类
继承 `BaseSettings` 并声明配置项，即可实现配置管理。

```python
from pydantic_settings import BaseSettings  # Pydantic v2 中移至 pydantic_settings 模块
from typing import List, Optional

class Settings(BaseSettings):
    # 基本类型配置，可指定默认值
    api_port: int = 8000  # 整数类型，默认 8000
    debug: bool = False   # 布尔类型，默认 False
    allowed_hosts: List[str] = ["localhost", "127.0.0.1"]  # 列表类型

    # 可选配置（允许为 None）
    database_url: Optional[str] = None

    # 从环境变量加载时，可自定义环境变量名（默认使用属性名的大写形式）
    redis_url: str = "redis://localhost:6379"
    class Config:
        # 自定义环境变量前缀（可选，加载时会自动拼接前缀，如 "MYAPP_REDIS_URL"）
        env_prefix = "myapp_"
        # 允许从 .env 文件加载配置（需指定文件路径）
        env_file = ".env"

# 实例化配置类（自动加载配置）
settings = Settings()

# 使用配置
print(f"API 端口: {settings.api_port}")
print(f"调试模式: {settings.debug}")
print(f"允许的主机: {settings.allowed_hosts}")
```


#### 3. 配置加载来源与优先级
`BaseSettings` 加载配置的优先级从高到低为：
1. **实例化时传入的参数**（如 `Settings(api_port=9000)`）。
2. **环境变量**（默认使用属性名的大写形式，如 `API_PORT`；若指定 `env_prefix`，则为 `MYAPP_API_PORT`）。
3. **`.env` 文件**（通过 `env_file` 指定，格式为 `KEY=VALUE`）。
4. **类中定义的默认值**。


#### 4. 从 `.env` 文件加载
创建 `.env` 文件（与代码同目录）：
```env
# .env 文件
MYAPP_API_PORT=8888
MYAPP_DEBUG=True
MYAPP_DATABASE_URL=postgresql://user:pass@localhost/db
```
实例化 `Settings` 时会自动读取 `.env` 中的配置，覆盖默认值。


#### 5. 类型转换示例
环境变量的值本质是字符串，但 `BaseSettings` 会自动转换为声明的类型：
- 环境变量 `MYAPP_DEBUG="True"` 会转换为 `bool(True)`。
- 环境变量 `MYAPP_ALLOWED_HOSTS="localhost,example.com"` 会转换为 `["localhost", "example.com"]`（列表类型自动按逗号分割）。
- 环境变量 `MYAPP_API_PORT="8080"` 会转换为 `int(8080)`。


#### 6. 验证与错误处理
如果配置值不符合类型要求（如将字符串赋值给 `int` 类型），实例化时会抛出 `ValidationError`，便于早期发现配置错误：
```python
# 若环境变量 MYAPP_API_PORT="abc"，则会报错：
# pydantic_core._pydantic_core.ValidationError: 1 validation error for Settings
# api_port
#   Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='abc', input_type=str]
```


### **总结**
`BaseSettings` 是 Pydantic 提供的强大配置管理工具，通过类定义简化了配置的声明、加载和验证流程，尤其适合需要从环境变量或 `.env` 文件读取配置的项目（如 Web 服务、数据处理脚本等）。其核心优势在于 **类型安全、多源加载、自动转换**，能显著提升配置管理的可靠性和开发效率。



## Celery 分布式任务队列

Celery 是一个基于 Python 的分布式任务队列（Distributed Task Queue），主要用于处理异步任务、定时任务和分布式任务调度。它通过将任务分发到多个工作节点（Worker）并行执行，提高系统的处理效率和可扩展性，尤其适合处理耗时操作（如数据处理、邮件发送、文件转换等）或需要定时执行的任务。


Celery 和 Python 的 `asyncio`（异步编程）都用于处理异步任务，但它们的设计理念、适用场景和实现方式有显著区别。以下从核心特性、适用场景、优缺点等方面进行对比：


### **1. 核心定位**
- **Celery**：  
  一个**分布式任务队列**，专注于**任务的异步调度、分发和执行**，支持跨进程、跨服务器的分布式部署。任务通常在独立的 Worker 进程中执行，通过消息代理（如 Redis、RabbitMQ）协调。

- **asyncio**：  
  Python 内置的**异步编程框架**，基于**事件循环（Event Loop）** 实现单线程内的并发，通过 `async/await` 语法简化异步代码，专注于**I/O 密集型任务的高效协作**（如网络请求、文件读写）。


### **2. 实现原理**
- **Celery**：  
  - 采用**多进程/多线程模型**（Worker 进程独立运行），任务通过消息队列（Broker）分发到不同 Worker 执行。  
  - 任务执行环境与主程序完全隔离（跨进程），因此支持 CPU 密集型任务（可利用多核）。  
  - 依赖外部中间件（Broker 和结果后端），架构相对复杂。

- **asyncio**：  
  - 基于**单线程事件循环**，通过非阻塞 I/O 和协程（Coroutine）实现并发，任务在同一线程内切换执行。  
  - 不适合 CPU 密集型任务（会阻塞事件循环，导致其他任务等待）。  
  - 无需外部依赖，纯 Python 内置模块，架构轻量。


### **3. 适用场景**
| **场景**                | **Celery 更适合**                          | **asyncio 更适合**                          |
|-------------------------|-------------------------------------------|--------------------------------------------|
| **任务类型**            | CPU 密集型（如数据计算、图像处理）         | I/O 密集型（如网络请求、数据库操作、API 调用） |
| **任务规模**            | 大规模、分布式任务（跨服务器执行）         | 小规模、单进程内的异步协作                   |
| **任务耗时**            | 长耗时任务（如小时级数据处理）             | 短耗时 I/O 任务（如毫秒级网络请求）          |
| **定时/周期性任务**     | 支持（内置 Beat 调度器，适合复杂定时规则） | 需额外库（如 `APScheduler`），原生支持有限   |
| **任务可靠性**          | 支持重试、结果存储、故障恢复               | 需手动处理重试和异常，可靠性依赖代码实现     |
| **跨语言协作**          | 支持（通过消息队列与其他语言通信）         | 仅限 Python 生态                            |


### **4. 总结：如何选择？**
- 若需要**分布式部署、处理 CPU 密集型任务、定时任务调度**，或任务需要**高可靠性和跨进程隔离**，选 **Celery**。  
- 若处理**高频 I/O 密集型任务**（如网络爬虫、API 服务），且希望**轻量、低开销**，选 **asyncio**。  
- 实际场景中，两者可结合使用：例如用 `asyncio` 处理单进程内的高并发 I/O，再通过 Celery 将复杂任务分发到分布式 Worker 处理。


### **核心组件**
1. **任务（Task）**  
   被异步执行的函数或方法，需用 Celery 装饰器（`@app.task`）定义。

2. **工作节点（Worker）**  
   负责执行任务的进程，可在多台服务器上启动，通过消息代理获取任务并执行。

3. **消息代理（Broker）**  
   接收任务并暂存，再分发给 Worker，是 Celery 的“任务调度中心”。常用选择：
   - **RabbitMQ**：功能完善，适合复杂场景（推荐）。
   - **Redis**：轻量高效，适合简单场景。

4. **结果后端（Backend）**  
   存储任务执行结果，供主程序查询。支持 Redis、数据库（SQLite/MySQL/PostgreSQL）等。


### **基本使用流程**
#### 1. 安装 Celery
需同时安装消息代理（如 Redis）和结果后端依赖：
```bash
pip install celery redis
```

#### 2. 定义任务（示例：`tasks.py`）
```python
from celery import Celery

# 初始化 Celery：指定 broker 和 backend
app = Celery(
    'my_tasks',  # 任务名
    broker='redis://localhost:6379/0',  # 消息代理（Redis）
    backend='redis://localhost:6379/1'  # 结果存储（Redis）
)

# 定义异步任务
@app.task
def add(x, y):
    return x + y

# 定义定时任务（通过装饰器指定执行时间）
@app.task
def send_reminder():
    print("发送定时提醒...")
```

#### 3. 启动 Worker
在终端运行，监听并执行任务：
```bash
celery -A tasks worker --loglevel=info
```
- `-A tasks`：指定 Celery 实例所在的模块（`tasks.py`）。
- `--loglevel=info`：输出日志级别。

#### 4. 调用任务
在主程序中异步调用任务：
```python
from tasks import add

# 异步执行任务，返回任务 ID
result = add.delay(2, 3)

# 查询任务状态和结果
print("任务 ID:", result.id)
print("任务状态:", result.status)  # PENDING/RUNNING/SUCCESS/FAILURE
print("任务结果:", result.get())  # 阻塞等待结果（实际中慎用，避免阻塞）
```

#### 5. 定时任务配置
通过 `beat_schedule` 配置定时任务，修改 `tasks.py`：
```python
app.conf.beat_schedule = {
    'send-reminder-every-minute': {
        'task': 'tasks.send_reminder',  # 任务路径
        'schedule': 60.0,  # 每 60 秒执行一次
    },
}
```
启动定时任务调度器（Beat）：
```bash
celery -A tasks beat --loglevel=info
```
（通常与 Worker 配合启动：`celery -A tasks worker --beat --loglevel=info`）


### **适用场景**
- 异步处理耗时操作（如图片/video 处理、数据分析）。
- 定时任务（如数据备份、报表生成、消息推送）。
- 分布式任务调度（如大规模爬虫、分布式计算）。
- 解耦系统模块（如订单提交后异步通知库存系统）。


Celery 凭借成熟的生态和灵活的配置，成为 Python 异步任务处理的主流选择，广泛应用于 Web 开发、数据分析等领域。

